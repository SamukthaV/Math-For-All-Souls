<!DOCTYPE html>
<html lang="en">
<head>
  <script defer data-domain="SamukthaV.github.io" src="https://plausible.io/js/script.js"></script>

    <style>
    .meta-bar { display:flex; gap:1rem; flex-wrap:wrap; align-items:center; margin:.75rem 0 1rem; color:#6b7280; font-size:.95rem }
    .pill { display:inline-flex; align-items:center; gap:.35rem; background:#f3f4f6; border:1px solid #e5e7eb; border-radius:999px; padding:.2rem .6rem }
    .pill b{ color:#111827 }
    .like-btn { border:1px solid #d1d5db; background:#fff; border-radius:.6rem; padding:.35rem .7rem; cursor:pointer }
    .like-btn.liked { background:#fde68a; border-color:#fbbf24 }
    .sep { color:#d1d5db }
    #comments { margin-top:1.25rem }
  </style>

  <meta name="post:title" content="Delving Deeper into the Spaces of Embeddings">
  <meta name="post:date"  content="2025-08-15">
  <meta name="post:tags"  content="embeddings,NLP,math">
  <meta name="post:hero"  content="../assets/blog/embedding_space/hero.webp"> 


  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Delving Deeper into the Spaces of Embeddings ‚Äî Math for All Souls</title>
  <meta name="description" content="A simple, math-first look at embeddings and semantic meaning." />

  <!-- MathJax (load after HTML) -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      startup: { typeset: true }
    };
  </script>
  <script id="MathJax-script" defer
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <style>
    :root { --bg:#f7f7fb; --fg:#222; --brand:#1f2937; --muted:#6b7280; }
    *{box-sizing:border-box}
    body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell;background:var(--bg);color:var(--fg)}
    header{background:linear-gradient(180deg,var(--brand),#374151);color:#fff;padding:2rem 1rem}
    header .wrap{max-width:960px;margin:0 auto}
    nav a{color:#fff;text-decoration:none;margin-right:1rem}
    main{max-width:800px;margin:2rem auto;padding:0 1rem;background:#fff;border:1px solid #e5e7eb;border-radius:12px;box-shadow:0 8px 24px rgba(0,0,0,.06)}
    article{padding:1.25rem 1.25rem 2rem;line-height:1.65}
    h1{margin:.2rem 0 0;font-size:1.9rem}
    .meta{color:var(--muted);margin-top:.35rem}
    blockquote{border-left:4px solid #2563eb;background:#f0f6ff;padding:.6rem 1rem;border-radius:6px}
    img{max-width:100%;height:auto;border-radius:10px;border:1px solid #e5e7eb}
    figure{margin:1.25rem 0;text-align:center}
    figcaption{font-size:.9rem;color:#555;margin-top:.45rem}
    .hr{height:1px;background:#e5e7eb;margin:1.25rem 0}
    a.back{display:inline-block;margin:1rem 0}
    code,pre{background:#f3f4f6;border-radius:6px}
    pre{padding:1rem;overflow:auto}
  </style>
</head>
<body>
  <header>
    <div class="wrap">
      <nav><a href="../index.html">Home</a> <a href="../blogs.html">Blogs</a> <a href="../papers.html">Papers</a></nav>
      <h1>Delving Deeper into the Spaces of Embeddings</h1>
      <p class="meta">By <strong>Samuktha</strong> ‚Ä¢ Math for All Souls</p>
      <div class="meta-bar">
      <span class="pill">üëÄ Views: <b id="pv">‚Äì</b></span>
      <span class="pill">üß≠ Unique: <b id="uv">‚Äì</b></span>
      <button id="like" class="like-btn">üëç Like <span id="likes">0</span></button>
    </div>
   
    </div>
  </header>

  <main>
    <article>
      <blockquote><em>Math is the key; Math is rigour ‚Äî unknown author</em></blockquote>

      <p>Embeddings are indispensable in current Large Language Models because they capture meaningful relationships between data, all backed by solid mathematical principles. In this article, we‚Äôll take a closer look at the math behind them in a simple and easy-to-understand way.</p>

      <h2>What do we mean by ‚Äúsemantic‚Äù?</h2>
      <p><strong>Semantic</strong> refers to the meaning or context behind the data rather than the exact words. Consider the two queries:</p>
      <ul>
        <li>‚ÄúBest AI models for image classification‚Äù</li>
        <li>‚ÄúTop neural networks for classifying images‚Äù</li>
      </ul>
      <p>Different words, same intent. With semantic understanding, a search engine treats them as the same kind of information need.</p>

      <h2>What is an embedding?</h2>
      <p>Formally, an embedding is produced by a function
        \( \varphi:\mathcal{X}\to\mathbb{R}^{N} \).
        For a sentence \( s \), its embedding is
        \( \mathbf{x}=\varphi(s)\in\mathbb{R}^{N} \).
        In many practical models, \( N=768 \), i.e.
        \( \mathbf{x}\in\mathbb{R}^{768} \).</p>

      <p>Because \( \mathbb{R}^{N} \) is a Euclidean space, we use the
        \( \ell_{2} \) norm
        \[
          \|\mathbf{x}\|_{2}=\Big(\sum_{i=1}^{N} x_{i}^{2}\Big)^{1/2}
        \]
        and define similarity via the cosine similarity:
        \[
          \operatorname{cos\_sim}(\mathbf{x},\mathbf{y})
          = \frac{\mathbf{x}^{\top}\mathbf{y}}{\|\mathbf{x}\|_{2}\,\|\mathbf{y}\|_{2}}.
        \]
        Larger cosine similarity (smaller angle) ‚áí more semantic relatedness.</p>

      <p>The locations of embeddings in the vector space carry <em>semantic</em> relations:
        similar meanings lie nearer; different meanings lie farther apart.</p>

      <figure>
        <!-- adjust the path to match your repo folders -->
        <img src="../assets/blog/embedding_space/hero.webp" alt="Points in 2D embedding space forming clusters for related words" />
        <figcaption>Fig. Depicting the semantic relation of embeddings in \( \mathbb{R}^{2} \).</figcaption>
      </figure>

      <p>As in the sketch above, words like <em>dog, puppy, canine, bite</em> cluster closely, while clusters such as <em>king, queen</em> and <em>jack, ace, spade</em> sit apart.</p>

      <h2>How are these embeddings generated?</h2>
      <p>Modern embeddings come from <strong>Transformers</strong>. They do not encode a word in isolation; attention layers let each token ‚Äúsee‚Äù other tokens in the sentence and capture context, so the same word can map to different vectors depending on usage.</p>

      <p>These networks are often tuned with <strong>contrastive learning</strong>: examples that should be similar (positive pairs) are pushed to have similar embeddings, while dissimilar examples (negative pairs) are pushed apart. Losses like triplet loss or InfoNCE make the geometry reflect meaning.</p>

      <div class="hr"></div>
      <p><em>Questions or suggestions? Write to <a href="mailto:mathforallsouls@gmail.com">mathforallsouls@gmail.com</a>.</em></p>
      <a class="back" href="../blogs.html">‚Üê Back to Blogs</a>
      <hr class="hr">
      <h2>Comments</h2>
      <div id="comments">
        <!-- Utterances embed (uses GitHub Issues on your repo). Ensure Issues are enabled. -->
        <script src="https://utteranc.es/client.js"
                repo="SamukthaV/Math-For-All-Souls"
                issue-term="pathname"
                label="üí¨ comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
        </script>
      </div>

    </article>
  </main>
<script>
  /* ---------- CONFIG ---------- */
  const NAMESPACE = "mathforallsouls";
  const PAGE_KEY  = "blog_embeddings"; // change per-page if you copy to other posts
  const COUNT_API = "https://api.countapi.xyz";

  const pvKey = `${NAMESPACE}:${PAGE_KEY}:views`;
  const uvKey = `${NAMESPACE}:${PAGE_KEY}:unique`;
  const lkKey = `${NAMESPACE}:${PAGE_KEY}:likes`;

  const seenFlag = `seen:${PAGE_KEY}`;
  const likeFlag = `liked:${PAGE_KEY}`;

  // Helpers
  async function ensure(key){
    try { await fetch(`${COUNT_API}/get/${key}`).then(r=>r.json()); }
    catch(_){
      // create with 0 if missing
      await fetch(`${COUNT_API}/create?namespace=${encodeURIComponent(NAMESPACE)}&key=${encodeURIComponent(key.split(":").slice(1).join(":"))}&value=0`);
    }
  }
  async function hit(key){
    try { const r = await fetch(`${COUNT_API}/hit/${key}`); return r.json(); }
    catch(e){ return { value: "‚Äì" }; }
  }
  async function get(key){
    try { const r = await fetch(`${COUNT_API}/get/${key}`); return r.json(); }
    catch(e){ return { value: "‚Äì" }; }
  }
  async function setIfMissing(key){
    await ensure(key);
    const data = await get(key);
    if (typeof data.value !== "number") {
      await fetch(`${COUNT_API}/set/${key}?value=0`);
    }
  }

  (async function initCounters(){
    // Make sure counters exist
    await Promise.all([setIfMissing(pvKey), setIfMissing(uvKey), setIfMissing(lkKey)]);

    // Page views: increment every load
    const pv = await hit(pvKey);
    document.getElementById("pv").textContent = pv.value ?? "‚Äì";

    // Unique: increment only once per browser
    if (!localStorage.getItem(seenFlag)) {
      const uv = await hit(uvKey);
      localStorage.setItem(seenFlag, "1");
      document.getElementById("uv").textContent = uv.value ?? "‚Äì";
    } else {
      const uv = await get(uvKey);
      document.getElementById("uv").textContent = uv.value ?? "‚Äì";
    }

    // Likes: initialize number and button state
    const lk = await get(lkKey);
    document.getElementById("likes").textContent = lk.value ?? 0;
    const likeBtn = document.getElementById("like");
    const liked = !!localStorage.getItem(likeFlag);
    if (liked) likeBtn.classList.add("liked");

    likeBtn.addEventListener("click", async ()=>{
      if (localStorage.getItem(likeFlag)) return; // already liked
      const next = await hit(lkKey);
      document.getElementById("likes").textContent = next.value ?? "‚Äì";
      localStorage.setItem(likeFlag, "1");
      likeBtn.classList.add("liked");
    });
  })();
</script>

</body>
</html>
